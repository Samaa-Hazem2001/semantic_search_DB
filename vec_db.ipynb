{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Nc0I94UY1CNM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "from sklearn.neighbors import BallTree\n",
        "import os.path\n",
        "import pickle\n",
        "import struct\n",
        "\n",
        "MAX_CLUSTER = 200\n",
        "CLUSTER_DIMENSION = 70\n",
        "MAX_CLUSTER_SECOND = 200\n",
        "MAX_CLUSTER_THIRD = 500\n",
        "\n",
        "############################################  final 12/10 : def cluste()    #############################################\n",
        "def store_disk(data,path):\n",
        "  #NOTE: data is list of lists not only a list\n",
        "  # with open(path, \"ab\") as fout:  # Use \"ab\" for appending in binary mode\n",
        "  with open(path, \"ab\") as fout:  # Use \"ab\" for appending in binary mode\n",
        "    for stored_list in data:\n",
        "      # Convert the ID and embedding to binary format\n",
        "      binary_data = struct.pack(f'{len(stored_list)}f', *stored_list)  # 'f' represents a float (4 bytes)\n",
        "      fout.write(binary_data)\n",
        "\n",
        "def store_disk_id(int_list , file_name):\n",
        "  # Convert the list of integers to a binary string\n",
        "  binary_data = struct.pack(f'{len(int_list)}i', *int_list)\n",
        "  # Write the binary data to the file\n",
        "  with open(file_name, \"wb\") as fout:  # Use \"wb\" for writing in binary mode\n",
        "    fout.write(binary_data)\n",
        "\n",
        "# initial is in global class, or saved in file\n",
        "# initialClusters is a dict, key is all first level clusters, each value is the second level cluster for each first level cluster\n",
        "# isInitialled is true if the first level clusters is build.\n",
        "# secondClusters is a dictionary with key = (first_level,second_level) and value = list of all ACTUAL IDS of the MAX_CLUSTER_THIRD inside this cluster\n",
        "# secondClusters size has to be = MAX_CLUSTER * MAX_CLUSTER_SECOND (for ex:200*200)\n",
        "# firstClustersDistances is first_level as the key and (70*values)-centroid- as the valule\n",
        "# secondClustersDistances is a dict of (first_level,second_level) as the key and (70*values)-the centroid- as the valule\n",
        "def cluster_data(data, initialClusters, secondClusters, firstClustersDistances, secondClustersDistances, thirdDistances, pre_path):\n",
        "  lastData = 0\n",
        "  dropped_data = []\n",
        "  i = 0\n",
        "  if len(initialClusters) <  MAX_CLUSTER:\n",
        "    for i in range(len(initialClusters), len(data)):\n",
        "      if lastData == MAX_CLUSTER: \n",
        "        break\n",
        "      if i > len(data) - 1:\n",
        "        # ball_tree = BallTree(initialClusters)\n",
        "        initialClusters_dictances =list( firstClustersDistances.values() )\n",
        "        ball_tree = BallTree(initialClusters_dictances)\n",
        "        with open(os.path.join(pre_path,'ball_tree_primary.pkl') , 'wb') as f:\n",
        "          pickle.dump(ball_tree, f)\n",
        "          # remove it from the RAM\n",
        "          del ball_tree\n",
        "\n",
        "        return \n",
        "\n",
        "      else:\n",
        "        if i > 0:\n",
        "          initialClusters_dictances =list( firstClustersDistances.values() )\n",
        "          ball_tree_tempPP = BallTree(initialClusters_dictances)\n",
        "          query_point_PP = np.reshape(data[i][1:],(1,CLUSTER_DIMENSION))\n",
        "          distances, indices = ball_tree_tempPP.query(query_point_PP, k = 1)\n",
        "          # distPP = calc_distPP(data[i][1:] , firstClustersDistances)\n",
        "          distPP = distances[0][0]\n",
        "          # print(\"distPP\",distPP)\n",
        "          if  distPP < 2.8 : #3.05: #3 : #5.12 : #(2**3): #(2**32): #(2**63)*(0.01):\n",
        "            dropped_data.append(data[i])\n",
        "            continue\n",
        "        initialClusters[lastData] = [data[i][0]]\n",
        "        firstClustersDistances[lastData] = data[i][1:]\n",
        "        #######\n",
        "        secondClustersDistances[(lastData,0)] = firstClustersDistances[lastData]\n",
        "        secondClusters[(lastData,0)]  = [data[i][0]] # <= m.s of that\n",
        "        thirdDistances[(lastData,0)]  = [data[i][1:]] # <= m.s of that\n",
        "        #####\n",
        "        lastData += 1\n",
        "\n",
        "    initialClusters_dictances =list( firstClustersDistances.values() )\n",
        "    ball_tree = BallTree(initialClusters_dictances)\n",
        "    with open(os.path.join(pre_path,'ball_tree_primary.pkl' ), 'wb') as f:\n",
        "      pickle.dump(ball_tree, f)\n",
        "      # remove it from the RAM\n",
        "      # del ball_tree\n",
        "  else:\n",
        "    with open(os.path.join(pre_path,'ball_tree_primary.pkl'), 'rb') as f:\n",
        "      ball_tree = pickle.load(f)\n",
        "\n",
        "\n",
        "  # later and important what is the range\n",
        "  all_drop_other = dropped_data + data[i:]\n",
        "  for data_item_index in range(len(all_drop_other)):\n",
        "    # should we change it to the avg or take for example the best 10 only\n",
        "    with open(os.path.join(pre_path,'ball_tree_primary.pkl'), 'rb') as f:\n",
        "      ball_tree = pickle.load(f)\n",
        "    query_point = np.reshape(all_drop_other[data_item_index][1:],(1,CLUSTER_DIMENSION))\n",
        "    distances, indices = ball_tree.query(query_point, k = MAX_CLUSTER)\n",
        "\n",
        "    for first in range(0, MAX_CLUSTER):\n",
        "      # current_nn = indices[first][first]\n",
        "      current_nn = indices[0][first]\n",
        "      list1 = initialClusters[current_nn]\n",
        "      # this is \"if\" not \"else if\"\n",
        "      if len(list1) <  MAX_CLUSTER_SECOND:\n",
        "        # initially the id of the first level cluster and second level will be the actuall , till we change them\n",
        "        list1.append(all_drop_other[data_item_index][0])\n",
        "\n",
        "        secondClustersDistances[(current_nn,len(list1)-1)] = all_drop_other[data_item_index][1:] #NOTE: len(list1)-1 not len(list1):as we did it after list1.append -m.s\n",
        "        # secondClusters[(current_nn,len(list1)-1)]  = []\n",
        "        secondClusters[(current_nn,len(list1)-1)]  = [all_drop_other[data_item_index][0]]\n",
        "        thirdDistances[(current_nn,len(list1)-1)]  = [all_drop_other[data_item_index][1:]]\n",
        "        #later + important : change the value of the item of firstClustersDistances[current_nn] to be \"new_clustered\"(new mean)\n",
        "        # firstClustersDistances[current_nn] = all_drop_other[data_item_index][1:]\n",
        "        old_first_center = firstClustersDistances[current_nn]\n",
        "        # old_first_center_numerator = old_first_center*(len(list1)-1)\n",
        "        old_first_center_numerator = [i *(len(list1)-1) for i in old_first_center]\n",
        "        new_first_center = [sum(x) for x in zip(old_first_center_numerator, all_drop_other[data_item_index][1:])]\n",
        "        firstClustersDistances[current_nn] = [i /(len(list1)) for i in new_first_center] #NOTE: len(list1) not len(list1)-1\n",
        "        # rebuild using the updated distance\n",
        "        initialClusters_dictances =list( firstClustersDistances.values() )\n",
        "        ball_tree = BallTree(initialClusters_dictances)\n",
        "        with open(os.path.join(pre_path,'ball_tree_primary.pkl') , 'wb') as f:\n",
        "          pickle.dump(ball_tree, f)\n",
        "\n",
        "\n",
        "        if len(list1) == MAX_CLUSTER_SECOND:\n",
        "          # indices_distances = [np.reshape(value,(1,CLUSTER_DIMENSION)) for key, value in secondClustersDistances.items() if key[0] == int(current_nn)]\n",
        "          indices_distances = [value for key, value in secondClustersDistances.items() if key[0] == int(current_nn)]\n",
        "          ball_tree_second = BallTree(indices_distances)\n",
        "          # path_second = 'ball_tree'+ str(current_nn) +'.pkl'\n",
        "          path_second = os.path.join(pre_path,('ball_tree'+ str(current_nn) +'.pkl'))\n",
        "          with open(path_second, 'wb') as f:\n",
        "            pickle.dump(ball_tree_second, f)\n",
        "            # remove it from the RAM\n",
        "            # del ball_tree_second\n",
        "        break\n",
        "\n",
        "\n",
        "      else:\n",
        "        path_second = os.path.join(pre_path, ('ball_tree'+ str(current_nn) +'.pkl'))\n",
        "        with open(path_second, 'rb') as f:\n",
        "          ball_tree_second = pickle.load(f)\n",
        "        distances_second, indices_second = ball_tree_second.query(query_point, k = MAX_CLUSTER_SECOND)\n",
        "\n",
        "        breking = False\n",
        "        for i in range(0, MAX_CLUSTER_SECOND): # <= wla a5leha range(0, len(indices_second[0]))\n",
        "          list2 = secondClusters[(current_nn,indices_second[0][i])]\n",
        "          listDist = thirdDistances[(current_nn,indices_second[0][i])]\n",
        "\n",
        "          # this is \"if\" not \"else if\"\n",
        "          if len(list2) <  MAX_CLUSTER_THIRD:\n",
        "            #list2 will have the ACTUAL IDS f3ln\n",
        "            list2.append(all_drop_other[data_item_index][0])\n",
        "            listDist.append(all_drop_other[data_item_index][1:])\n",
        "            breking = True\n",
        "\n",
        "            # #########   update secondClustersDistances   ##########\n",
        "            # #later + important : change the value of the item of secondClustersDistances[(current_nn,first)] to be \"new_clustered\"(new mean)\n",
        "            # # econdClustersDistances[(current_nn,i)] = all_drop_other[data_item_index][1:]\n",
        "            # # secondClustersDistances[(current_nn,indices_second[0][i])] = all_drop_other[data_item_index][1:]\n",
        "            # old_center = secondClustersDistances[(current_nn,indices_second[0][i])]\n",
        "            # # old_center_numerator = old_center*(len(list2)-1)\n",
        "            # old_center_numerator = [j *(len(list2)-1) for j in old_center]\n",
        "            # new_center = [sum(x) for x in zip(old_center_numerator, all_drop_other[data_item_index][1:] )]\n",
        "            # secondClustersDistances[(current_nn,indices_second[0][i])] = [iii /(len(list2)) for iii in new_center] #NOTE: len(list2) not len(list2)-1\n",
        "            # # rebuild using the updated distance\n",
        "            # indices_distances = [value for key, value in secondClustersDistances.items() if key[0] == int(current_nn)]\n",
        "            # ball_tree_second = BallTree(indices_distances)\n",
        "            # # path_second = 'ball_tree'+ str(current_nn) +'.pkl'\n",
        "            # path_second = os.path.join(pre_path, ('ball_tree'+ str(current_nn) +'.pkl'))\n",
        "            # with open(path_second, 'wb') as f:\n",
        "            #   pickle.dump(ball_tree_second, f)\n",
        "\n",
        "\n",
        "            # #########   update firstClustersDistances   ##########\n",
        "            # # old_primary_center = firstClustersDistances[current_nn]*len(list1) #later: m.s:len(list1)\n",
        "            # old_primary_center = firstClustersDistances[current_nn]\n",
        "            # old_primary_center_num = [jj *len(list1) for jj in old_primary_center]\n",
        "            # # old_primary_center_num = [sub(x) for x in zip(old_primary_center_num, old_center)]\n",
        "            # zipped = [*zip(old_primary_center_num, old_center)]\n",
        "            # old_primary_center_num = [(x-y) for (x,y) in zipped]\n",
        "            # old_primary_center_num = [sum(x) for x in zip(secondClustersDistances[(current_nn,indices_second[0][i])], old_primary_center_num)] #add the new centroid\n",
        "            # firstClustersDistances[current_nn] = [ll /len(list1) for ll in old_primary_center_num]\n",
        "            # # rebuild using the updated distance\n",
        "            # initialClusters_dictances =list( firstClustersDistances.values() )\n",
        "            # ball_tree = BallTree(initialClusters_dictances)\n",
        "            # with open(os.path.join(pre_path, ('ball_tree_primary.pkl' )), 'wb') as f:\n",
        "            #   pickle.dump(ball_tree, f)\n",
        "\n",
        "            break\n",
        "\n",
        "        if breking == True:\n",
        "          break\n",
        "\n",
        "\n",
        "  kk = -1\n",
        "  for initialClusters_item in initialClusters.items():\n",
        "    kk += 1\n",
        "    key_id , value_list = initialClusters_item\n",
        "    # if(len(value_list) < MAX_CLUSTER_SECOND )\n",
        "    if(len(value_list) > 0 and len(value_list) < MAX_CLUSTER_SECOND ):\n",
        "      indices_distances = [value for key, value in secondClustersDistances.items() if key[0] == kk ]\n",
        "      ball_tree_second = BallTree(indices_distances)\n",
        "      # path_second = 'ball_tree'+ str(kk) +'.pkl'\n",
        "      path_second = os.path.join(pre_path, ('ball_tree'+ str(kk) +'.pkl'))\n",
        "      with open(path_second, 'wb') as f:\n",
        "        pickle.dump(ball_tree_second, f)\n",
        "        # remove it from the RAM\n",
        "        del ball_tree_second\n",
        "\n",
        "  for secondClusters_item in secondClusters.items():\n",
        "    # list2 = secondClusters[(current_nn,indices_second[0][i])]\n",
        "    key , val = secondClusters_item\n",
        "    #if there is any item in the cluster\n",
        "    if len(val) > 0 :\n",
        "      list_id = val\n",
        "      primary_level = key[0]\n",
        "      secondry_level = key[1]\n",
        "      # path_second_ids = 'ID' + str(primary_level) + '_' + str(secondry_level)+'.bin'\n",
        "      path_second_ids = os.path.join(pre_path, ('ID' + str(primary_level) + '_' + str(secondry_level)+'.bin'))\n",
        "      store_disk_id(list_id,path_second_ids)\n",
        "\n",
        "\n",
        "  # allIDs_Third = 0\n",
        "  for thirdDistances_item in thirdDistances.items():\n",
        "    key , val = thirdDistances_item\n",
        "    #if there is any item in the cluster\n",
        "    # allIDs_Third += len(val)\n",
        "    if len(val) > 0 :\n",
        "      list_dist = val\n",
        "      primary_level = key[0]\n",
        "      secondry_level = key[1]\n",
        "      path_second = os.path.join(pre_path, (str(primary_level) + '_' + str(secondry_level)+'.bin'))\n",
        "      store_disk(list_dist,path_second)\n",
        "\n",
        "  return \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "9uOqfGh71CNP"
      },
      "outputs": [],
      "source": [
        "\n",
        "########################   search   #######################\n",
        "import os.path\n",
        "import numpy as np\n",
        "import struct\n",
        "# import os\n",
        "CLUSTER_DIMENSION = 70\n",
        "\n",
        "def calculate_distances(records, given_record):\n",
        "    distances = []\n",
        "\n",
        "    for record in records:\n",
        "        # Calculate Euclidean distance between each record and the given record\n",
        "        distance = np.linalg.norm(np.array(record) - np.array(given_record))\n",
        "        distances.append(distance)\n",
        "    return distances\n",
        "\n",
        "def calc_2(records, given_record):\n",
        "    all_cal_dist = records.dot(given_record.T).T / (np.linalg.norm(records, axis=1) * np.linalg.norm(given_record))\n",
        "    return all_cal_dist\n",
        "\n",
        "def Leaf_Brute_Force(records, given_record):\n",
        "    all_dist = calculate_distances(records, given_record)\n",
        "    dist_dict = {i: row for i, row in enumerate(all_dist)}\n",
        "    # temp_dict = {k: v for k, v in zip(flatten_ids,flatten_dist)}\n",
        "    sorty = dict(sorted(dist_dict.items(), key=lambda x: x[1]))\n",
        "    ret_indices = list(sorty.keys())\n",
        "    ret_distances = list(sorty.values())\n",
        "    return ret_distances,ret_indices\n",
        "\n",
        "def _cal_score( vec1, vec2):\n",
        "  dot_product = np.dot(vec1, vec2)\n",
        "  norm_vec1 = np.linalg.norm(vec1)\n",
        "  norm_vec2 = np.linalg.norm(vec2)\n",
        "  cosine_similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "  return cosine_similarity\n",
        "\n",
        "def retrive22(dataset , query , top_k = 5):\n",
        "  scores = []\n",
        "  # with open(file_path, \"r\") as fin:\n",
        "  i = 0\n",
        "  for row in dataset:\n",
        "    score = _cal_score(query, row)\n",
        "    scores.append((score, i))\n",
        "    i+= 1\n",
        "  # here we assume that if two rows have the same score, return the lowest ID\n",
        "  scores = sorted(scores, reverse=True)[:top_k]\n",
        "  allIds = [s[1] for s in scores]\n",
        "  allDist = [s[0][0] for s in scores]\n",
        "  return allDist,allIds\n",
        "\n",
        "\n",
        "def read_bin(path):\n",
        "  lists = []\n",
        "  with open(path, \"rb\") as fin:  # Use \"rb\" for reading in binary mode\n",
        "    binary_data = fin.read()\n",
        "\n",
        "    # Unpack the binary data into a list of floats\n",
        "    num_elements = len(binary_data) // struct.calcsize('f')\n",
        "    unpacked_data = struct.unpack(f'{num_elements}f', binary_data)\n",
        "\n",
        "  unpacked_data = list(unpacked_data)\n",
        "  num_of_lists = int(len(unpacked_data)/CLUSTER_DIMENSION)\n",
        "  lists = np.array(unpacked_data).reshape((num_of_lists,CLUSTER_DIMENSION))\n",
        "  return lists\n",
        "\n",
        "def read_bin_id(file_name):\n",
        "  ## Read the binary data from the file\n",
        "  with open(file_name, \"rb\") as fin:  # Use \"rb\" for reading in binary mode\n",
        "    binary_data = fin.read()\n",
        "  # Unpack the binary data into a list of integers\n",
        "  num_elements = len(binary_data) // struct.calcsize('i')\n",
        "  unpacked_data = struct.unpack(f'{num_elements}i', binary_data)\n",
        "  return list(unpacked_data)\n",
        "\n",
        "def append_or_extend(container, item, req3):\n",
        "  if hasattr(item, '__iter__'):  # Check if item is iterable\n",
        "      container.extend(item)\n",
        "      req3 += len(item)\n",
        "  else:\n",
        "      container.append([item])\n",
        "      req3 += 1\n",
        "\n",
        "def team_search(query_node,TOP_K , initialClusters , pre_path):\n",
        "  # print(\"pre_path for seach \", pre_path)\n",
        "  ef = 10 #10 # 10\n",
        "  # path = './ball_tree_primary.pkl'\n",
        "  path = os.path.join(pre_path, './ball_tree_primary.pkl')\n",
        "  check_file = os.path.isfile(path)\n",
        "  # allNodes is the array of actuall ids\n",
        "  allNodesActIds = []\n",
        "  allNodesdist = []\n",
        "  if check_file:\n",
        "    with open(path, 'rb') as f:\n",
        "      ball_tree = pickle.load(f)\n",
        "    tree_array = ball_tree.get_arrays()\n",
        "    primary_len = len(tree_array[1])\n",
        "\n",
        "    if primary_len <= TOP_K :\n",
        "      # allNodes = [i for i in range(0,primary_len)]\n",
        "      allNodesActIds.append( [i for i in range(0,primary_len)] )\n",
        "      return allNodesActIds #//uncommet it later\n",
        "\n",
        "    # req_nodes =min(primary_len,TOP_K .. later\n",
        "    distances_111 , indices_111  = ball_tree.query(query_node, k = min(ef**3,primary_len))\n",
        "\n",
        "    req2 = 0\n",
        "    for nnIndex_1 in range(len(indices_111[0])):\n",
        "      if req2 >= ef**3 : # m.s later\n",
        "        break\n",
        "      current_nn_111 = indices_111[0][nnIndex_1]\n",
        "      # path_second = 'ball_tree'+ str(current_nn_111) +'.pkl'\n",
        "      path_second = os.path.join(pre_path, ('ball_tree'+ str(current_nn_111) +'.pkl'))\n",
        "      check_file_second = os.path.isfile(path_second)\n",
        "      if check_file_second:\n",
        "        with open(path_second, 'rb') as f:\n",
        "          ball_tree_second = pickle.load(f)\n",
        "        list222 = initialClusters[current_nn_111]\n",
        "        distances_222 , indices_222  = ball_tree_second.query(query_node, k = min((ef**2), len(list222)))\n",
        "        req3 = 0\n",
        "        for nnIndex_2 in range(len(indices_222[0])):\n",
        "          if req3 >= ef**2 : # m.s later\n",
        "            break\n",
        "          current_nn_222 = indices_222[0][nnIndex_2]\n",
        "          # path_third = str(current_nn_111) + '_' + str(current_nn_222)+'.bin'\n",
        "          # path_third_ids = 'ID' + str(current_nn_111) + '_' + str(current_nn_222)+'.bin'\n",
        "          path_third = os.path.join(pre_path, (str(current_nn_111) + '_' + str(current_nn_222)+'.bin'))\n",
        "          path_third_ids = os.path.join(pre_path, ('ID' + str(current_nn_111) + '_' + str(current_nn_222)+'.bin'))\n",
        "          check_file_third = os.path.isfile(path_third)\n",
        "          check_file_third_ids = os.path.isfile(path_third_ids)\n",
        "\n",
        "          if check_file_third and check_file_third_ids:\n",
        "            ThirdDist = read_bin(path_third)\n",
        "            list_actual = read_bin_id(path_third_ids)\n",
        "            # print('ThirdDist in serach ',ThirdDist , 'for key = ',(current_nn_111,current_nn_222))\n",
        "            #to deal with if we want ef nodes but the leaf has less than ef nodes\n",
        "            take3 = min(ef,len(list_actual))\n",
        "            dist_third_sort , ind_third_sort = retrive22(ThirdDist, query_node, top_k= take3)\n",
        "            dist_third_sort = dist_third_sort[:take3]\n",
        "            ind_third_sort = ind_third_sort[:take3]\n",
        "            final_actual = [list_actual[least] for least in ind_third_sort]\n",
        "            allNodesActIds.append(final_actual)\n",
        "            allNodesdist.append(dist_third_sort)\n",
        "            req3 += take3\n",
        "          # if there is no tree, then this cluster \"(current_nn_111,current_nn_222)\" of the secondry level has only one node\n",
        "          # then return it\n",
        "          else:\n",
        "            print(\"a primary_secondry not exist ----------\")\n",
        "            temp_list = initialClusters[current_nn_111]\n",
        "            append_or_extend(allNodesActIds, temp_list[current_nn_222],req3)\n",
        "            # break\n",
        "\n",
        "        req2 += req3 # add \"req3\" (which the nodes collected from this cluster \"current_nn_111\")\n",
        "      # if there is no tree, then this cluster \"current_nn_111\" of the primary level has only one node\n",
        "      # then return it\n",
        "      else:\n",
        "        allNodesActIds.append(initialClusters[current_nn_111])\n",
        "        req2 += len(initialClusters[current_nn_111])\n",
        "  else:\n",
        "    print(' \"ball_tree_primary.pkl\" is not exist' )\n",
        "\n",
        "  flatten_ids = sum(allNodesActIds, [])\n",
        "  flatten_dist = sum(allNodesdist, [])\n",
        "  temp_dict = {k: v for k, v in zip(flatten_ids,flatten_dist)}\n",
        "  sorty = dict(sorted(temp_dict.items(), key=lambda x: x[1], reverse=True))\n",
        "  sorty_keys = list(sorty.keys())[:TOP_K]\n",
        "  # print('my ids = ' , sorty_keys )\n",
        "  return sorty_keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-TDzuazN1CNQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Dict, List, Annotated\n",
        "import os\n",
        "import shutil\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "MAX_CLUSTER = 200\n",
        "CLUSTER_DIMENSION = 70\n",
        "MAX_CLUSTER_SECOND = 200\n",
        "MAX_CLUSTER_THIRD = 500\n",
        "class VecDB:\n",
        "  # def __init__(self, file_path = \"saved_db.csv\", new_db = True) -> None:\n",
        "  def __init__(self, file_path = '0'  , new_db = True) -> None:\n",
        "    self.last_folder = file_path\n",
        "    self.file_initialClusters = 'initialClusters.bin'\n",
        "    self.file_secondClusters = 'secondClusters.bin'\n",
        "    self.file_firstClustersDistances = 'firstClustersDistances.bin'\n",
        "    self.file_secondClustersDistances = 'secondClustersDistances.bin'\n",
        "    self.file_thirdDistances = 'thirdDistances.bin'\n",
        "    # self.last_len = 0\n",
        "    self.DBfile_path = \"saved_db.csv\"\n",
        "\n",
        "    if new_db: # <= later: remove this condition ?\n",
        "      self.create_folder()\n",
        "      self.create_empty_file(os.path.join(self.last_folder, self.file_initialClusters))\n",
        "      self.create_empty_file(os.path.join(self.last_folder, self.file_secondClusters))\n",
        "      self.create_empty_file(os.path.join(self.last_folder, self.file_firstClustersDistances))\n",
        "      self.create_empty_file(os.path.join(self.last_folder, self.file_secondClustersDistances))\n",
        "      self.create_empty_file(os.path.join(self.last_folder, self.file_thirdDistances))\n",
        "\n",
        "      # storing the original database\n",
        "      self.create_empty_file(os.path.join(self.last_folder, self.DBfile_path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def insert_records(self, rows: List[Dict[int, Annotated[List[float], 70]]]):\n",
        "    # records_np_recovery = np.array([list(rec.values())[1] for rec in rows])\n",
        "    records_np_recovery = [[list(rec.values())[0],*list(rec.values())[1]] for rec in rows]\n",
        "    # make a copy from last folder\n",
        "    new_folder = str( len(records_np_recovery) + int(self.last_folder) )\n",
        "    self.copy_folder(new_folder)\n",
        "    # print('after copyyyyy',self.last_folder)\n",
        "\n",
        "    # ab+: Opens a file for both appending and reading in binary mode.\n",
        "    with open(os.path.join(self.last_folder, self.DBfile_path), \"ab+\") as fout:  # Use \"ab\" for appending in binary mode\n",
        "      for row in rows:\n",
        "        id, embed = row[\"id\"], row[\"embed\"]\n",
        "        # Convert the ID and embedding to binary format\n",
        "        id_binary = struct.pack('I', id)  # Assuming 'I' represents an unsigned int (4 bytes)\n",
        "        embed_binary = struct.pack(f'{len(embed)}f', *embed)  # 'f' represents a float (4 bytes)\n",
        "        # Concatenate the binary data and write it to the file\n",
        "        fout.write(id_binary + embed_binary)\n",
        "    # self._build_index()\n",
        "    del rows\n",
        "    self._build_index(records_np_recovery)\n",
        "\n",
        "  def _build_index(self,data):\n",
        "\n",
        "    # read the old info\n",
        "    initialClusters = self.read_dict(os.path.join(self.last_folder, self.file_initialClusters))\n",
        "    secondClusters = self.read_dict(os.path.join(self.last_folder, self.file_secondClusters))\n",
        "    firstClustersDistances = self.read_dict(os.path.join(self.last_folder, self.file_firstClustersDistances))\n",
        "    secondClustersDistances  = self.read_dict(os.path.join(self.last_folder, self.file_secondClustersDistances))\n",
        "    thirdDistances = self.read_dict(os.path.join(self.last_folder, self.file_thirdDistances))\n",
        "\n",
        "    pre_path = self.last_folder\n",
        "    cluster_data(data, initialClusters, secondClusters, firstClustersDistances, secondClustersDistances,thirdDistances,pre_path)\n",
        "\n",
        "    del data\n",
        "    #### write the new info\n",
        "    self.store_dict(os.path.join(self.last_folder, self.file_initialClusters) , initialClusters)\n",
        "    self.store_dict(os.path.join(self.last_folder, self.file_secondClusters) , secondClusters)\n",
        "    self.store_dict(os.path.join(self.last_folder, self.file_firstClustersDistances) , firstClustersDistances)\n",
        "    self.store_dict(os.path.join(self.last_folder, self.file_secondClustersDistances) , secondClustersDistances)\n",
        "    # self.store_dict(os.path.join(self.last_folder, self.file_thirdDistances) , thirdDistances)\n",
        "    thirdDistances = {key: [] for key in thirdDistances}\n",
        "    self.store_dict(os.path.join(self.last_folder, self.file_thirdDistances) , thirdDistances)\n",
        "\n",
        "    return\n",
        "\n",
        "  def retrive(self, query , top_k):\n",
        "    pre_path = self.last_folder\n",
        "    initialClusters = self.read_dict(os.path.join(pre_path , self.file_initialClusters))\n",
        "    query_trancate = query.astype(np.float32)\n",
        "    ids_result = team_search(query_trancate ,top_k , initialClusters, pre_path)\n",
        "    return ids_result\n",
        "\n",
        "#   def debug_results(self,actual_ids):\n",
        "#     for item in self.secondClusters.items():\n",
        "#       key , val = item\n",
        "#       for oneVal in val:\n",
        "#         if oneVal in actual_ids:\n",
        "#           print(key)\n",
        "\n",
        "#   def print_all(self):\n",
        "#     print(len(self.secondClusters))\n",
        "#     print('data' , self.data)\n",
        "\n",
        "#################### file system folders functions ##################\n",
        "  def create_empty_file(self,file_path):\n",
        "    with open(file_path, 'w'):\n",
        "        pass\n",
        "\n",
        "  def create_folder(self):\n",
        "    # os.makedirs(self.last_folder)\n",
        "    os.makedirs(self.last_folder, exist_ok=True) #<= later: check \"exist_ok\"\n",
        "\n",
        "  def copy_folder(self,new_folder_path):\n",
        "    if os.path.exists(self.last_folder): # and !os.path.exists(new_folder_path):\n",
        "        shutil.copytree(self.last_folder, new_folder_path)\n",
        "        # print('copy : ',self.last_folder,'to ', new_folder_path)\n",
        "        self.last_folder = new_folder_path\n",
        "        return new_folder_path\n",
        "    # else:\n",
        "    #     os.makedirs(folder_path, exist_ok=True)\n",
        "    #     return folder_path\n",
        "\n",
        "  # def store_file(self,data, file_path):\n",
        "  #     file_path = os.path.join(self.last_folder, file_path)\n",
        "  #     print('now storing to ',file_path)\n",
        "  #     with open(file_path, 'w') as file:\n",
        "  #         file.write(data)\n",
        "\n",
        "  def read_dict(self,file_path):\n",
        "    # with open(file_path, 'r') as file:\n",
        "    #   data = json.load(file)\n",
        "    loaded_data = {}\n",
        "    if os.path.getsize(file_path) > 0:\n",
        "      with open(file_path, 'rb') as file:\n",
        "        loaded_data = pickle.load(file)\n",
        "    return loaded_data\n",
        "\n",
        "  def store_dict(self,file_path,data):\n",
        "    # NOTE: the mode here is 'w' as we want to override the old one\n",
        "    # Store the dictionary in binary format\n",
        "    with open(file_path, 'wb') as file:\n",
        "      pickle.dump(data, file)\n",
        "\n",
        "  # def store_dict_Third(self,file_path,data):\n",
        "  #   # NOTE: the mode here is 'w' as we want to override the old one\n",
        "  #   # Store the dictionary in binary format\n",
        "  #   with open(file_path, 'wb') as file:\n",
        "  #     pickle.dump(data, file)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BPUwY8CQ1CNR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# from worst_case_implementation import VecDBWorst\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "AVG_OVERX_ROWS = 10\n",
        "\n",
        "@dataclass\n",
        "class Result:\n",
        "    run_time: float\n",
        "    top_k: int\n",
        "    db_ids: List[int]\n",
        "    actual_ids: List[int]\n",
        "\n",
        "results = []\n",
        "to_print_arr = []\n",
        "\n",
        "def memory_usage_run_queries(args):\n",
        "    global results\n",
        "    # This part is added to calcauate the RAM usage\n",
        "    mem_before = max(memory_usage())\n",
        "    mem = memory_usage(proc=(run_queries, args, {}), interval = 1e-3)\n",
        "    return results, max(mem) - mem_before\n",
        "\n",
        "def run_queries_old(db, np_rows, top_k, num_runs):\n",
        "    results = []\n",
        "    QUERY_SEED_NUMBER = 10\n",
        "    rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "    query = rng.random((1, 70), dtype=np.float32)\n",
        "    for _ in range(num_runs):\n",
        "        # query = np.random.random((1,70))#.astype(np.float32)\n",
        "\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrive(query, top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "\n",
        "        tic = time.time()\n",
        "        actual_ids = np.argsort(np_rows.dot(query.T).T / (np.linalg.norm(np_rows, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]\n",
        "        toc = time.time()\n",
        "        np_run_time = toc - tic\n",
        "\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
        "\n",
        "        # for debug:\n",
        "        # print(\"actual_ids from TA code\",actual_ids[:(top_k*3)])\n",
        "        # db.debug_results(actual_ids[:(top_k*3)])\n",
        "\n",
        "    return results\n",
        "\n",
        "def run_queries(db, query, top_k, actual_ids, num_runs):\n",
        "    global results\n",
        "    results = []\n",
        "    for _ in range(num_runs):\n",
        "        tic = time.time()\n",
        "        db_ids = db.retrive(query, top_k)\n",
        "        toc = time.time()\n",
        "        run_time = toc - tic\n",
        "        results.append(Result(run_time, top_k, db_ids, actual_ids))\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_result(results: List[Result]):\n",
        "    # scores are negative. So getting 0 is the best score.\n",
        "    scores = []\n",
        "    run_time = []\n",
        "    for res in results:\n",
        "        run_time.append(res.run_time)\n",
        "        # case for retireving number not equal to top_k, socre will be the lowest\n",
        "        if len(set(res.db_ids)) != res.top_k or len(res.db_ids) != res.top_k:\n",
        "            scores.append( -1 * len(res.actual_ids) * res.top_k)\n",
        "            continue\n",
        "        score = 0\n",
        "        for id in res.db_ids:\n",
        "            try:\n",
        "                ind = res.actual_ids.index(id)\n",
        "                if ind > res.top_k * 3:\n",
        "                    score -= ind\n",
        "            except:\n",
        "                score -= len(res.actual_ids)\n",
        "        scores.append(score)\n",
        "\n",
        "    return sum(scores) / len(scores), sum(run_time) / len(run_time)\n",
        "\n",
        "\n",
        "def get_actual_ids_first_k(actual_sorted_ids, k):\n",
        "    return [id for id in actual_sorted_ids if id < k]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ###############################  main #######################\n",
        "# if __name__ == \"__main__\":\n",
        "#     db = VecDB()\n",
        "#     rng = np.random.default_rng(50)\n",
        "#     ########################### 10k  ###########################\n",
        "#     records_np = rng.random((10000, 70), dtype=np.float32)\n",
        "#     records_dict = [{\"id\": i, \"embed\": list(row)} for i, row in enumerate(records_np)]\n",
        "#     _len = len(records_np)\n",
        "#     del records_np\n",
        "#     db.insert_records(records_dict)\n",
        "#     # res = run_queries_old(db, records_np, 5, 10)\n",
        "#     # print(evaluate_result(res))\n",
        "\n",
        "#     ########################### 100k  ###########################\n",
        "#     records_np_new = rng.random((90000, 70), dtype=np.float32)\n",
        "#     # records_np = np.concatenate([records_np, records_np_new])\n",
        "#     # del records_np_new\n",
        "#     # rng = np.random.default_rng(50)\n",
        "#     # records_np = rng.random((100000, 70), dtype=np.float32)\n",
        "#     records_dict = [{\"id\": i + _len, \"embed\": list(row)} for i, row in enumerate(records_np_new)]\n",
        "#     _len += len(records_np_new)\n",
        "#     del records_np_new\n",
        "#     db.insert_records(records_dict)\n",
        "#     # res = run_queries_old(db, records_np, 5, 10)\n",
        "#     # print(evaluate_result(res))\n",
        "\n",
        "#     # ########################### 1M  ###########################\n",
        "#     # records_np_new = rng.random((900000, 70), dtype=np.float32)\n",
        "#     # # records_np = np.concatenate([records_np, records_np_new])\n",
        "#     # # del records_np_new\n",
        "#     # # rng = np.random.default_rng(50)\n",
        "#     # # records_np = rng.random((1000000, 70), dtype=np.float32)\n",
        "#     # records_dict = [{\"id\": i + _len, \"embed\": list(row)} for i, row in enumerate(records_np_new)]\n",
        "#     # _len += len(records_np_new)\n",
        "#     # del records_np_new\n",
        "#     # db.insert_records(records_dict)\n",
        "#     # # res = run_queries_old(db, records_np, 5, 10)\n",
        "#     # # print(evaluate_result(res))\n",
        "\n",
        "#     # ########################### 5M + 10M + 15M + 20M  ###########################\n",
        "#     # for _ in range(19):\n",
        "#     #     records_np_new = rng.random((1000000, 70), dtype=np.float32)\n",
        "#     #     # records_np = np.concatenate([records_np, records_np_new])\n",
        "#     #     # del records_np_new\n",
        "#     #     # rng = np.random.default_rng(50)\n",
        "#     #     # records_np = rng.random((3000000, 70), dtype=np.float32)\n",
        "#     #     records_dict = [{\"id\": i + _len, \"embed\": list(row)} for i, row in enumerate(records_np_new)]\n",
        "#     #     _len += len(records_np_new)\n",
        "#     #     del records_np_new\n",
        "#     #     db.insert_records(records_dict)\n",
        "#     #     # res = run_queries_old(db, records_np, 5, 10)\n",
        "#     #     # print(evaluate_result(res))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we are in  1 M\n",
            "we are in  2 M\n",
            "we are in  3 M\n",
            "we are in  4 M\n",
            "we are in  5 M\n",
            "we are in  6 M\n",
            "we are in  7 M\n",
            "we are in  8 M\n",
            "we are in  9 M\n",
            "we are in  10 M\n",
            "we are in  11 M\n",
            "we are in  12 M\n",
            "we are in  13 M\n",
            "we are in  14 M\n"
          ]
        }
      ],
      "source": [
        "# ###############################  main #######################\n",
        "# if __name__ == \"__main__\":\n",
        "#     # db = VecDB()\n",
        "#     rng = np.random.default_rng(50)\n",
        "    \n",
        "#     records_np = rng.random((3000000, 70), dtype=np.float32)\n",
        "#     _len = len(records_np)\n",
        "#     del records_np\n",
        "\n",
        "#     records_np = rng.random((3000000, 70), dtype=np.float32)\n",
        "#     _len += len(records_np)\n",
        "#     del records_np\n",
        "\n",
        "#     db = VecDB(file_path = '6000000' , new_db = False)\n",
        "\n",
        "#     # ########################### 10M + 15M + 20M  ###########################\n",
        "#     for i in range(14):\n",
        "#         print('we are in ', i+1 , 'M')\n",
        "#         records_np_new = rng.random((1000000, 70), dtype=np.float32)\n",
        "#         # records_np = np.concatenate([records_np, records_np_new])\n",
        "#         # del records_np_new\n",
        "#         # rng = np.random.default_rng(50)\n",
        "#         # records_np = rng.random((3000000, 70), dtype=np.float32)\n",
        "#         records_dict = [{\"id\": i + _len, \"embed\": list(row)} for i, row in enumerate(records_np_new)]\n",
        "#         _len += len(records_np_new)\n",
        "#         del records_np_new\n",
        "#         db.insert_records(records_dict)\n",
        "#         # res = run_queries_old(db, records_np, 5, 10)\n",
        "#         # print(evaluate_result(res))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U747Hajs1CNR"
      },
      "outputs": [],
      "source": [
        "# # import numpy as np\n",
        "# # rng = np.random.default_rng(50)\n",
        "# # vectors = rng.random((10**7*2, 70), dtype=np.float32)\n",
        "\n",
        "# QUERY_SEED_NUMBER = 30\n",
        "# rng = np.random.default_rng(QUERY_SEED_NUMBER)\n",
        "# query = rng.random((1, 70), dtype=np.float32)\n",
        "# actual_sorted_ids_20m = np.argsort(vectors.dot(query.T).T / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(query)), axis= 1).squeeze().tolist()[::-1]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onTNTO6T1CNS"
      },
      "outputs": [],
      "source": [
        "# db = VecDB(file_path = '10000', new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, (10**4))\n",
        "# # print('TA ida',actual_ids)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"10K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)\n",
        "\n",
        "# db = VecDB(file_path = '100000', new_db = False)\n",
        "# actual_ids = get_actual_ids_first_k(actual_sorted_ids_20m, (10**5))\n",
        "# # print('TA ida',actual_ids)\n",
        "# res = run_queries(db, query, 5, actual_ids, 1)  # one run to make everything fresh and loaded\n",
        "# res, mem = memory_usage_run_queries((db, query, 5, actual_ids, 3)) # actual runs to compute time, and memory\n",
        "# eval = evaluate_result(res)\n",
        "# to_print = f\"100K\\tscore\\t{eval[0]}\\ttime\\t{eval[1]:.2f}\\tRAM\\t{mem:.2f} MB\"\n",
        "# to_print_arr.append(to_print)\n",
        "# print(to_print)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKLIsv5I1CNS"
      },
      "outputs": [],
      "source": [
        "# pip install nbconvert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJBwpbtD1CNS"
      },
      "outputs": [],
      "source": [
        "# pip install memory_profiler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ui6NC8i1F_4"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.neighbors import BallTree\n",
        "# import pickle\n",
        "# import struct\n",
        "\n",
        "# def store_disk(data,path):\n",
        "#   #NOTE: data is list of lists not only a list\n",
        "#   # with open(path, \"ab\") as fout:  # Use \"ab\" for appending in binary mode\n",
        "#   with open(path, \"ab\") as fout:  # Use \"ab\" for appending in binary mode\n",
        "#     for stored_list in data:\n",
        "#       # Convert the ID and embedding to binary format\n",
        "#       binary_data = struct.pack(f'{len(stored_list)}f', *stored_list)  # 'f' represents a float (4 bytes)\n",
        "#       fout.write(binary_data)\n",
        "\n",
        "# def read_bin(path):\n",
        "#   lists = []\n",
        "#   with open(path, \"rb\") as fin:  # Use \"rb\" for reading in binary mode\n",
        "#     binary_data = fin.read()\n",
        "\n",
        "#     # Unpack the binary data into a list of floats\n",
        "#     num_elements = len(binary_data) // struct.calcsize('f')\n",
        "#     unpacked_data = struct.unpack(f'{num_elements}f', binary_data)\n",
        "\n",
        "#   unpacked_data = list(unpacked_data)\n",
        "#   num_of_lists = int(len(unpacked_data)/70)\n",
        "#   lists = np.array(unpacked_data).reshape((num_of_lists,70))\n",
        "#   return lists\n",
        "\n",
        "# list1 = [ [1,2,3],\n",
        "#     [84,5,687]\n",
        "# ]\n",
        "# # store_disk(list1,'test.bin')\n",
        "# # print(read_bin('test.bin'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFc3U-Ts13yR"
      },
      "outputs": [],
      "source": [
        "# rng = np.random.default_rng(50)\n",
        "#     ########################### 10k  ###########################\n",
        "# records_np = rng.random((20, 2), dtype=np.float32)\n",
        "# print(records_np[0])\n",
        "# # rng = np.random.default_rng(50)\n",
        "# del records_np\n",
        "# records_np = rng.random((20, 2), dtype=np.float32)\n",
        "# print(records_np[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63ljuDkc3EOm"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# def read_dict(file_path):\n",
        "#     # with open(file_path, 'r') as file:\n",
        "#     #   data = json.load(file)\n",
        "#     loaded_data = {}\n",
        "#     if os.path.getsize(file_path) > 0:\n",
        "#         with open(file_path, 'rb') as file:\n",
        "#             loaded_data = pickle.load(file)\n",
        "#     return loaded_data\n",
        "# temp_dict = read_dict('./5000000/secondClusters.bin')\n",
        "# # print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# def read_dict(file_path):\n",
        "#     # with open(file_path, 'r') as file:\n",
        "#     #   data = json.load(file)\n",
        "#     loaded_data = {}\n",
        "#     if os.path.getsize(file_path) > 0:\n",
        "#         with open(file_path, 'rb') as file:\n",
        "#             loaded_data = pickle.load(file)\n",
        "#     return loaded_data\n",
        "# temp_dict = read_dict('./5000000/secondClusters.bin')\n",
        "# # print()\n",
        "\n",
        "# def store_dict(file_path,data):\n",
        "#     # NOTE: the mode here is 'w' as we want to override the old one\n",
        "#     # Store the dictionary in binary format\n",
        "#     with open(file_path, 'wb') as file:\n",
        "#         pickle.dump(data, file)\n",
        "# thirdDistances = {key: [] for key in temp_dict}\n",
        "# # print('thirdDistances',thirdDistances)\n",
        "# store_dict('thirdDistances.bin', thirdDistances)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
